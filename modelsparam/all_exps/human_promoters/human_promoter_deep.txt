input_channels|4|#number of original input channels (for DNA = 4)
use_embeddings|False|#Use embeddings if True otherwise use one-hot encoded input
embd_window|5|#embedding window size
embd_size|50|#embedding size word2vec model
embd_kmersize|3|#size of kmer in word2vec model
num_multiheads|8|#numMultiHeads #8,4
singlehead_size|32|#SingleHeadSize #32
multihead_size|1024|#MultiHeadSize
use_pooling|False|#Use pooling at single head level
pooling_val|4|#if use pooling at single head level, size of pooling
readout_strategy|normalize|#read out layer type/strategy#normalize
use_RNN|False|#use RNN in the model
RNN_hiddensize|100|#RNN hidden size
use_CNN|True|#use CNN layer
CNN1_useexponential|False|#use Exponential function as an activation in the first CNN layer (ReLU is used if False)
use_posEnc|False|#use positional encoding
CNN_filters|[200]|#number of CNN layer filters
CNN_filtersize|[13]|#size of the CNN filter
use_CNNpool|True|#use CNN pooling
CNN_poolsize|6|#CNN maxpool size
CNN_padding|6|#CNN padding, need to be determined based on input length and filter size
get_CNNout|True|#get first CNN layer output (useful for motif analysis)
get_seqs|True|#get sequences for the test set examples (useful for motif analysis)
get_pattn|True|#get Attention value
batch_size|64|#batch size
num_epochs|100|#number of epochs
relativeAttn|False|#Use relativeAttn or not
Learnable_relativeAttn|False|#Learn relativeAttn or not
mixed_attn|False|#Mix attention or nor
mixed_attn_config|0.25|#How much use Relative Attention
sqrt_dist|False|##Sqrt of Relative Attention
num_attnlayers|1|#number of attention layers
multiple_linear|False| #number of linear layers after attention layer
entropy_loss|False|#include entropy loss
entropy_reg_value|0.01|#entropy regularization value
residual_connection|False|##
exp_name|human_promoters|#name of experiment
cnn_attention_block|False|
linear_layer_size|512|
sei|False|#use sei after attention
sei_bn|True|Use BN in sei
